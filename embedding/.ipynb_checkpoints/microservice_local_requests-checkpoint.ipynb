{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with the Nemo Retriever \"Embedding\" microservice\n",
    "|rkharwar@nvidia.com| Author(s) | [Ruchika Kharwar](https://github.com/rasalt)\n",
    "\n",
    "NOTE: This notebook has been tested in the following environment:\n",
    "Python version = 3.10.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "## Objective\n",
    "\n",
    "## Before you begin\n",
    "\n",
    "\n",
    "### Set up your environment.\n",
    "Refer to page <> for details on how to deploy the service.\n",
    "You should have a docker service running namely \n",
    "\"embedding-ms\" on the port of your choice. \n",
    "For the purpose of this exercise this service was deployed on port 8080.\n",
    "\n",
    "eg. In my environment this service is running as \"embedding-ms-alone\" on port 8080\n",
    "7f8e5cb76cb2   nvcr.io/ohlfw0olaadg/ea-participants/nemo-retriever-embedding-microservice:24.02   \"/opt/nvidia/nvidia_â€¦\"   15 minutes ago   Up 15 minutes             0.0.0.0:8080->8080/tcp, :::8080->8080/tcp                                                                                                            embedding-ms-alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the environment vairables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"http://localhost:8080/v1/embeddings\"\n",
    "MODEL_ID = \"NV-Embed-QA\"\n",
    "INPUT_TYPE = \"passage\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a sample text string which we will vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The girl threw the butter out of the window to see 'butter-fly'.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the embedding container API to generate embedding for the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../utils/')\n",
    "\n",
    "from request_utils import *\n",
    "\n",
    "url = \"http://localhost:8080/v1\"\n",
    "model_id = \"NV-Embed-QA\"\n",
    "input_type = \"passage\"\n",
    "\n",
    "embed = {\n",
    "  \"input\": [text],\n",
    "  \"model\": MODEL_ID,\n",
    "  \"input_type\": INPUT_TYPE\n",
    "}\n",
    "\n",
    "response = post_api(URL, embed)\n",
    "passage_embeddings = [embedding['embedding'] for embedding in response['data']]\n",
    "print(len(passage_embeddings))\n",
    "print(passage_embeddings[:1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use FAISS as our vector store. We will create the index and add the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "index = faiss.IndexFlatL2(len(passage_embeddings[0]))\n",
    "index.add(np.array(passage_embeddings).astype('float32'))\n",
    "\n",
    "print(\"Number of passages in index:\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our passages added to our index, let's get the vector embedding for our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"who is wearing blue pants and a yellow hat?\"\n",
    "\n",
    "embed = {\n",
    "  \"input\": query,\n",
    "  \"model\": model_id,\n",
    "  \"input_type\": \"query\"\n",
    "}\n",
    "\n",
    "response = post_api(url+\"/embeddings\", embed)\n",
    "query_embedding = [response['data'][0]['embedding']]\n",
    "print(query_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's search our index for passages that are related to our query vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = num_passages\n",
    "\n",
    "distances, ndxs = index.search(np.array(query_embedding).astype('float32'), topk)\n",
    "\n",
    "print('Distances:\\n', distances)\n",
    "print('Ordered Indices:\\n', ndxs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's take a look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Query:\", query)\n",
    "print(\"Nearest Passage:\", passages[ndxs[0][0]])\n",
    "print(\"Furthest Passage:\", passages[ndxs[0][len(ndxs[0])-1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
