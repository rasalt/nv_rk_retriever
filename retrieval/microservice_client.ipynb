{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nemo Retrieval Microservice Tutorial\n",
    "\n",
    "This notebook provides a simple end-to-end example of how to use Nemo Retreiver Microservice APIs.\n",
    "# Getting Started with the Nemo Retriever \"Retriever\" microservice\n",
    "|nickr@nvidia.com| Author(s) | [Nick Remearan](https://github.com/)\n",
    "|rkharwar@nvidia.com| Author(s) | [Ruchika Kharwar](https://github.com/rasalt)\n",
    "\n",
    "NOTE: This notebook has been tested in the following environment:\n",
    "Python version = 3.10.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines \n",
    "\n",
    "A pipeline is an end-to-end retrieval function using Nvidia Retriever Microservice.\n",
    "This system is accessed via a set of API calls/Client library\n",
    "\n",
    "Here we list the pipeline names along with their status and the embedding model the pipeline is using. Notice the document store being used on the backend is part of the pipeline name. \n",
    "\n",
    "There are other properties of the pipelines (chunking strategy) which can also be viewed by printing out the entire pipeline object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "<> \n",
    "\n",
    "## Objective\n",
    "This notebook aims to show you how to leverage a freshly deployed \"embedding micro-service\".\n",
    "These examples aim to be building blocks of the larger solution you will likley have in place for yout Generative AI use case.\n",
    "\n",
    "## Before you begin\n",
    "### Set up your environment.\n",
    "Refer to page <> for details on how to deploy the service.\n",
    "You should have docker services running in your environment thus  \n",
    "\n",
    "docker                         dockerd                        dockerd-rootless-setuptool.sh  dockerd-rootless.sh            docker-proxy                   \n",
    "nvidia@dev-h100-rkharwar-gpu01:~/retriever_03182024/docker-compose$ docker compose -f \n",
    "config/                      docker-compose-ea.yaml       docker-compose-nemollm.yaml  models/                      models_orig/                 volumes/                     \n",
    "nvidia@dev-h100-rkharwar-gpu01:~/retriever_03182024/docker-compose$ docker compose -f docker-compose-ea.yaml ps\n",
    "NAME                              IMAGE                                                                              COMMAND                  SERVICE          CREATED        STATUS                    PORTS\n",
    "docker-compose-elasticsearch-1    docker.elastic.co/elasticsearch/elasticsearch:8.12.0                               \"/bin/tini -- /usr/l…\"   elasticsearch    21 hours ago   Up 21 hours (healthy)     0.0.0.0:9200->9200/tcp, :::9200->9200/tcp, 9300/tcp\n",
    "docker-compose-embedding-ms-1     nvcr.io/ohlfw0olaadg/ea-participants/nemo-retriever-embedding-microservice:24.02   \"/opt/nvidia/nvidia_…\"   embedding-ms     21 hours ago   Up 21 hours (healthy)     \n",
    "docker-compose-etcd-1             quay.io/coreos/etcd:v3.5.11                                                        \"etcd -advertise-cli…\"   etcd             21 hours ago   Up 21 hours (healthy)     2379-2380/tcp\n",
    "docker-compose-milvus-1           milvusdb/milvus:v2.3.5                                                             \"/tini -- milvus run…\"   milvus           21 hours ago   Up 21 hours (healthy)     \n",
    "docker-compose-minio-1            minio/minio:RELEASE.2023-03-20T20-16-18Z                                           \"/usr/bin/docker-ent…\"   minio            21 hours ago   Up 21 hours (healthy)     9000/tcp\n",
    "docker-compose-otel-collector-1   otel/opentelemetry-collector-contrib:0.91.0                                        \"/otelcol-contrib --…\"   otel-collector   21 hours ago   Up 21 hours               0.0.0.0:4317->4317/tcp, :::4317->4317/tcp, 0.0.0.0:13133->13133/tcp, :::13133->13133/tcp, 0.0.0.0:55679->55679/tcp, :::55679->55679/tcp, 55678/tcp\n",
    "docker-compose-postgres-1         postgres:16.1                                                                      \"docker-entrypoint.s…\"   postgres         21 hours ago   Up 21 hours               0.0.0.0:5432->5432/tcp, :::5432->5432/tcp\n",
    "docker-compose-retrieval-ms-1     nvcr.io/ohlfw0olaadg/ea-participants/nemo-retriever-microservice:24.02             \"/usr/bin/shelless_u…\"   retrieval-ms     21 hours ago   Up 21 hours (unhealthy)   0.0.0.0:1984->8000/tcp, :::1984->8000/tcp\n",
    "docker-compose-tika-1             apache/tika:2.9.1.0                                                                \"/bin/sh -c 'exec ja…\"   tika             21 hours ago   Up 21 hours               0.0.0.0:9998->9998/tcp, :::9998->9998/tcp\n",
    "docker-compose-zipkin-1           openzipkin/zipkin:3.0.6                                                            \"start-zipkin\"           zipkin           21 hours ago   Up 21 hours (healthy)     9410/tcp, 0.0.0.0:9411->9411/tcp, :::9411->9411/tcp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the environment vairables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_URL = \"http://localhost:1984/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../utils/')\n",
    "\n",
    "from request_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "NeMo Retriever comes preloaded with several pipelines. Pipelines fully encapsulate the indexing and query logic from chunking to embedding model specifics to which vector store and index to use, and more.\n",
    "\n",
    "In this section we see the types and configuration settings of the pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_api(PIPELINE_URL + \"/pipelines\")\n",
    "\n",
    "for pipeline in response['pipelines']:\n",
    "    print(pipeline)\n",
    "    print(pipeline['id'])\n",
    "    print(\"\\tenabled=\" + str(pipeline['enabled']))\n",
    "    print(\"\\tmodel=\" + str(pipeline['config']['index']['pipeline']['components']['embedder']['init_parameters']))\n",
    "    print(\"\\tmodel=\" + str(pipeline['config']['index']['pipeline']['components']['splitter']))\n",
    "    print(\"-\" * 80)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collections\n",
    "\n",
    "A collection refers to a set of uploaded documents. Using collections allows us to query against different subsets of documents.\n",
    "\n",
    "Before we create a collection, let's specify a few items we will need:\n",
    "1. Collection name\n",
    "2. Pipeline type for the collection (we saw our options in the previous step)\n",
    "3. Document to add to the collection\n",
    "4. Query we want to do retrieval on\n",
    "5. Number of chunks we want retrieved for our query (top k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download a document of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -O \"CUDA.pdf\" -nc --user-agent=\"Mozilla\" https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup the environment vairables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"cuda\"\n",
    "PIPELINE_TYPE = \"hybrid\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we create our collection using the pipeline type we specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = {\n",
    "    \"name\": COLLECTION_NAME,\n",
    "    \"pipeline\": PIPELINE_TYPE\n",
    "}\n",
    "\n",
    "response = post_api(PIPELINE_URL+\"/collections\", collection)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check to make sure our collection was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = response['collection']['id']\n",
    "response = get_api(PIPELINE_URL + \"/collections/\" + id)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a document to the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"CUDA.pdf\"\n",
    "response = upload_doc(PIPELINE_URL, id, [f\"name={document}\"], document)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's query our retrieval pipeline now on the document we just added and request topk chunks to be returned to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the __global__ execution space specifier?\"\n",
    "topk = 5\n",
    "\n",
    "retrieve = {\n",
    "    \"query\": query,\n",
    "    \"top_k\": topk\n",
    "}\n",
    "response = post_api(PIPELINE_URL + \"/collections/\" + id + \"/search\", retrieve)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_chunk = \"\"\n",
    "for chunk in response['chunks']:\n",
    "    print(f\"chunk len (chars): {len(chunk['content'])}\")\n",
    "    large_chunk += f\" {chunk}\"\n",
    "\n",
    "print(f\"context len (chars): {len(large_chunk)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#invoke_url = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/8f4118ba-60a8-4e6b-8574-e38a4067a4a3\" #Mixtral 8x7B\n",
    "NVCF_BASE_URL = \"https://integrate.api.nvidia.com/v1\"\n",
    "NGC_API_KEY = \"\"\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "  base_url = NVCF_BASE_URL,\n",
    "  api_key = NGC_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = (\n",
    " \"<s>[INST] <<SYS>>\"\n",
    " \"{system_prompt}\"\n",
    " \"<</SYS>>\"\n",
    " \"\"\n",
    " \"[Question]\"\n",
    " \"{question}\"\n",
    " \"[The Start of the Reference Context]\"\n",
    " \"{ctx_ref}\"\n",
    " \"[The End of Reference Context][/INST]\"\n",
    ")\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful AI assistant.\n",
    "You will reply to questions only based on the reference context that you are provided.\n",
    "If you cannot answer the question using only the reference context then you will politely respond that you are unable to answer the question given the provided information.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "try:\n",
    "    prompt = PROMPT_TEMPLATE.format(system_prompt=system_prompt, question=query, ctx_ref=large_chunk)\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "            \"content\": prompt,\n",
    "            \"role\": \"user\"\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.7,\n",
    "        \"max_tokens\": 500,\n",
    "        \"stream\": True\n",
    "        }\n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"mistralai/mixtral-8x7b-instruct-v0.1\",\n",
    "      messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "      temperature=0.5,\n",
    "      top_p=1,\n",
    "      max_tokens=1024,\n",
    "      stream=True\n",
    "    )\n",
    "    for chunk in completion:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            print(chunk.choices[0].delta.content, end=\"\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, let's clean up by removing any collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in completion:\n",
    "  if chunk.choices[0].delta.content is not None:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")\n",
    "\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            print(line)\n",
    "            json_part = line.decode('utf-8').split(\"data: \", 1)[1]\n",
    "            data = json.loads(json_part)\n",
    "            print(data)\n",
    "            print(data[\"choices\"][0][\"delta\"][\"content\"], end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'collections': [{'pipeline': 'hybrid', 'name': 'testCollection', 'id': 'c869ce4e-f73a-4f56-bc70-c9effb59f9e6'}, {'pipeline': 'hybrid', 'name': 'testCollection', 'id': '24ad6245-6ad0-46f4-8d3b-d935df57480d'}, {'pipeline': 'hybrid', 'name': 'testCollection', 'id': '85238fa8-e78f-4d64-b6b2-0f5efe1e2068'}, {'pipeline': 'hybrid', 'name': 'testCollection', 'id': 'daf90a93-6a1c-407d-9241-d4a5cac11e7f'}]}\n",
      "{'collections': []}\n"
     ]
    }
   ],
   "source": [
    "response = get_api(PIPELINE_URL + \"/collections\")\n",
    "print(response)\n",
    "\n",
    "for collection in response['collections']:\n",
    "    delete_api(PIPELINE_URL + \"/collections/\"+collection['id'])\n",
    "\n",
    "response = get_api(PIPELINE_URL + \"/collections\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
